# -*- coding: utf-8 -*-
"""bone_age_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C2s2Tybp0ONKl_4dmkUCd1yihAJ1K7Jo

This notebook is an attempt to predict bone age using Xception(pre trained model)<br>
"""

from google.colab import drive
drive.mount('/content/drive')

# Libraries used

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tensorflow as tf
import datetime, os
import math
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

"""**Some Setup**<br>
The cell below creates the pandas dataframes for training and testing.
"""

#loading dataframes
train_df = pd.read_csv('/content/drive/MyDrive/Dataset/train_dataset.csv')
test_df = pd.read_csv('/content/drive/MyDrive/Dataset/test_dataset.csv')

#appending file extension to id column for both training and testing dataframes
train_df['id'] = train_df['id'].apply(lambda x: str(x)+'.jpg')
test_df['id'] = test_df['id'].apply(lambda x: str(x)+'.jpg')

train_df.head()

"""**Some EDA and feature engineering follow**"""

#finding out the number of male and female children in the dataset
#creating a new column called gender to keep the gender of the child as a string
train_df['gender'] = train_df['male'].apply(lambda x: 'male' if x else 'female')
print(train_df['gender'].value_counts())
sns.countplot(x = train_df['gender'])

#oldest child in the dataset
print('MAX age: ' + str(train_df['boneage'].max()) + ' months')

#youngest child in the dataset
print('MIN age: ' + str(train_df['boneage'].min()) + ' months')

#mean age is
mean_bone_age = train_df['boneage'].mean()
print('mean: ' + str(mean_bone_age))

#median bone age
print('median: ' +str(train_df['boneage'].median()))

#standard deviation of boneage
std_bone_age = train_df['boneage'].std()

#models perform better when features are normalised to have zero mean and unity standard deviation
#using z score for the training
train_df['bone_age_z'] = (train_df['boneage'] - mean_bone_age)/(std_bone_age)
print("Important Parameters")
print(mean_bone_age)
print(std_bone_age)
print(train_df.head())

#plotting a histogram for bone ages
train_df['boneage'].hist(color = 'green')
plt.xlabel('Age in months')
plt.ylabel('Number of children')
plt.title('Number of children in each age group')

train_df['bone_age_z'].hist(color = 'violet')
plt.xlabel('bone age z score')
plt.ylabel('Number of children')
plt.title('Relationship between number of children and bone age z score')

#Relationship between age and gender with a categorical scatter plot (swarmplot)
sns.swarmplot(x = train_df['gender'], y = train_df['boneage'])

#distribution of age within each gender
male = train_df[train_df['gender'] == 'male']
female = train_df[train_df['gender'] == 'female']
fig, ax = plt.subplots(2,1)
ax[0].hist(male['boneage'], color = 'blue')
ax[0].set_ylabel('Number of Males')
ax[1].hist(female['boneage'], color = 'red')
ax[1].set_xlabel('Age in months')
ax[1].set_ylabel('Number of Females')
fig.set_size_inches((10,7))

#splitting train dataframe into traininng and validation dataframes
df_train, df_valid = train_test_split(train_df, test_size = 0.2, random_state = 0)

"""Looking into the dataset..."""

import matplotlib.image as mpimg
for filename, boneage, gender in train_df[['id','boneage','gender']].sample(4).values:
    img = mpimg.imread('/content/drive/MyDrive/Dataset/compressed-dataset/'+ filename)
    plt.imshow(img)
    plt.title('Image name:{}  Bone age: {} years  Gender: {}'.format(filename, boneage/12, gender))
    plt.axis('off')
    plt.show()

"""**Setting up Image Data Generators!**<br>
We use image data generators for both training, testing and preprocessing of images. Validation set is already broken off from training set.
"""

#library required for image preprocessing
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from  keras.applications.xception import preprocess_input

#reducing down the size of the image
img_size = 256

train_data_generator = ImageDataGenerator(preprocessing_function = preprocess_input)
val_data_generator = ImageDataGenerator(preprocessing_function = preprocess_input)

#train data generator
train_generator = train_data_generator.flow_from_dataframe(
    dataframe = df_train,
    directory = '/content/drive/MyDrive/Dataset/compressed-dataset',
    x_col= 'id',
    y_col= 'bone_age_z',
    batch_size = 32,
    seed = 42,
    shuffle = True,
    class_mode= 'other',
    flip_vertical = True,
    color_mode = 'rgb',
    target_size = (img_size, img_size))

#validation data generator
val_generator = val_data_generator.flow_from_dataframe(
    dataframe = df_valid,
    directory = '/content/drive/MyDrive/Dataset/compressed-dataset',
    x_col = 'id',
    y_col = 'bone_age_z',
    batch_size = 32,
    seed = 42,
    shuffle = True,
    class_mode = 'other',
    flip_vertical = True,
    color_mode = 'rgb',
    target_size = (img_size, img_size))

#test data generator
test_data_generator = ImageDataGenerator(preprocessing_function = preprocess_input)

test_generator = test_data_generator.flow_from_dataframe(
    test_df,
    directory = '/content/drive/MyDrive/Dataset/compressed-test-dataset',
    x_col = 'id',
    shuffle = True,
    class_mode = None,
    color_mode = 'rgb',
    target_size = (img_size,img_size))

test_X, test_Y = next(val_data_generator.flow_from_dataframe(
                            df_valid,
                            directory = '/content/drive/MyDrive/AI Project/Dataset/compressed-dataset',
                            x_col = 'id',
                            y_col = 'bone_age_z',
                            target_size = (img_size, img_size),
                            batch_size = 2523,
                            class_mode = 'other'
                            ))

""" The function to plot training and validation error as a function of epochs"""

def plot_it(history):
    '''function to plot training and validation error'''
    fig, ax = plt.subplots( figsize=(20,10))
    ax.plot(history.history['mae_in_months'])
    ax.plot(history.history['val_mae_in_months'])
    plt.title('Model Error')
    plt.ylabel('error')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='upper right')
    ax.grid(color='black')
    plt.show()

from keras.metrics import mean_absolute_error
def mae_in_months(x_p, y_p):
    '''function to return mae in months'''
    return mean_absolute_error((std_bone_age*x_p + mean_bone_age), (std_bone_age*y_p + mean_bone_age))

from tensorflow.keras.layers import GlobalMaxPooling2D, Dense,Flatten
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint,EarlyStopping,ReduceLROnPlateau
from tensorflow.keras import Sequential

model_1 = tf.keras.applications.xception.Xception(input_shape = (img_size, img_size, 3),
                                           include_top = False,
                                           weights = 'imagenet')
model_1.trainable = True
model_2 = Sequential()
model_2.add(model_1)
model_2.add(GlobalMaxPooling2D())
model_2.add(Flatten())
model_2.add(Dense(10, activation = 'relu'))
model_2.add(Dense(1, activation = 'linear'))


#compile model
model_2.compile(loss ='mse', optimizer= 'adam', metrics = [mae_in_months] )

#model summary
model_2.summary()

# Commented out IPython magic to ensure Python compatibility.

# Load the TensorBoard notebook extension
# %load_ext tensorboard
logs_dir = '/content/drive/MyDrive/AI Project/Log'
# %tensorboard --port 6007 --logdir "{logs_dir}"

"""Evaluating the best saved model on the validation data and visualising results!!"""

model_2.load_weights('/content/drive/MyDrive/AI Project/Model/best_model.h5')
pred = mean_bone_age + std_bone_age*(model_2.predict(test_X, batch_size = 32, verbose = True))
test_months = mean_bone_age + std_bone_age*(test_Y)

ord_ind = np.argsort(test_Y)
ord_ind = ord_ind[np.linspace(0, len(ord_ind)-1, 8).astype(int)] # take 8 evenly spaced ones
fig, axs = plt.subplots(4, 2, figsize = (15, 30))
for (ind, ax) in zip(ord_ind, axs.flatten()):
    ax.imshow(test_X[ind, :,:,0], cmap = 'bone')
    ax.set_title('Age: %fY\nPredicted Age: %fY' % (test_months[ind]/12.0,
                                                           pred[ind]/12.0))
    ax.axis('off')
fig.savefig('trained_image_predictions.png', dpi = 300)

fig, ax = plt.subplots(figsize = (7,7))
ax.plot(test_months, pred, 'r.', label = 'predictions')
ax.plot(test_months, test_months, 'b-', label = 'actual')
ax.legend(loc = 'upper right')
ax.set_xlabel('Actual Age (Months)')
ax.set_ylabel('Predicted Age (Months)')

"""**The plot deviates from the line at very old and very young ages probably because we have less examples for those cases in the dataset**

Predicting on test data, we obtain:
"""

test_generator.reset()
y_pred = model_2.predict(test_generator)
predicted = y_pred.flatten()
predicted_months = 127.3207517246848 + 41.18202139939618*(predicted)
filenames=test_generator.filenames
results=pd.DataFrame({"Filename":filenames,
                      "Predictions": predicted_months/12.0})
results.to_csv("results_test.csv",index=False)

from tensorflow.keras.optimizers import Optimizer

# Define a function for fine-tuning
def fine_tune(model, lr, metric):
    '''
    Unfreezes last 2 convolutional blocks of base model and re-compiles model
    Allows for adjusting of learning rate

    Parameters
    ----------
    model: previously compiled and trained model
    lr: new learning rate (lower)
    metric: error metric

    Returns
    ----------
    Re-compiled model with partially unfrozen convolutional base
    '''
    # Unfreeze last 2 convolutional blocks of base model
    model_1.trainable = True
    for i, layer in enumerate(model_1.layers):
        if i < 115:
            layer.trainable = False
        else:
            layer.trainable = True

    # Lower learning rate
    optim = tf.keras.optimizers.legacy.Adam(
        learning_rate=lr,
        beta_1=0.9,
        beta_2=0.999
    )

    # Re-compile model
    model.compile(loss='mse', optimizer=optim, metrics=metric)

    return model
#early stopping
early_stopping = EarlyStopping(monitor='val_loss',
                              min_delta=0,
                              patience= 5,
                              verbose=0, mode='auto')

#model checkpoint
mc = ModelCheckpoint('D:/AI Project/Model/best_model5.h5', monitor='val_loss', mode='min', save_best_only=True)
logdir = os.path.join(logs_dir,datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
tensorboard_callback =  TensorBoard(logdir, histogram_freq = 1)

#reduce lr on plateau
red_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)

callbacks = [tensorboard_callback,early_stopping,mc, red_lr_plat]
callbacks = [tensorboard_callback,early_stopping,mc, red_lr_plat]
# Fine-tune the model
fine_tuned_model = fine_tune(model_2, lr=0.0001, metric=[mae_in_months])

# Train the fine-tuned model
fine_tuned_history = fine_tuned_model.fit_generator(
    train_generator,
    steps_per_epoch=315,
    validation_data=val_generator,
    validation_steps=1,
    epochs=20,
    callbacks=callbacks
)

# Split the gender column into binary values (0 or 1)
train_df['gender'] = train_df['gender'].apply(lambda x: 1 if x == 'male' else 0)

# Split the data into training and validation sets
df_train, df_valid = train_test_split(train_df, test_size=0.2, random_state=0)

# Image data generators
train_data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)
val_data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)



train_image_generator = train_data_generator.flow_from_dataframe(
    dataframe=df_train,
    directory='/content/drive/MyDrive/AI Project/Dataset/compressed-dataset',
    x_col='id',
    y_col='bone_age_z',
    batch_size=32,
    seed=42,
    shuffle=True,
    class_mode='other',
    flip_vertical=True,
    color_mode='rgb',
    target_size=(img_size, img_size))

val_image_generator = val_data_generator.flow_from_dataframe(
    dataframe=df_valid,
    directory='/content/drive/MyDrive/AI Project/Dataset/compressed-dataset',
    x_col='id',
    y_col='bone_age_z',
    batch_size=32,
    seed=42,
    shuffle=True,
    class_mode='other',
    flip_vertical=True,
    color_mode='rgb',
    target_size=(img_size, img_size))

train_subset = df_train.sample(frac=0.05, random_state=42)  # Select 5% of the data randomly

train_image_generator_subset = train_data_generator.flow_from_dataframe(
    dataframe=train_subset,
    directory='/content/drive/MyDrive/AI Project/Dataset/compressed-dataset',
    x_col='id',
    y_col='bone_age_z',
    batch_size=32,
    seed=42,
    shuffle=True,
    class_mode='other',
    flip_vertical=True,
    color_mode='rgb',
    target_size=(img_size, img_size))

batch_size = 32

def custom_data_generator_subset(image_generator, gender_values, target_values, batch_size):
    while True:
        batch_images, _ = next(image_generator)
        batch_genders = gender_values[:batch_size]
        batch_targets = target_values[:batch_size]

        yield [batch_images, batch_genders], batch_targets

# Use the subset of training data for the custom data generator
custom_train_generator = custom_data_generator_subset(train_image_generator_subset,
                                                      train_subset['gender'].values,
                                                      train_image_generator_subset.next()[1],
                                                      batch_size)

# Define a custom data generator
def custom_data_generator(image_generator, gender_values, target_values, batch_size):
    while True:
        batch_images, batch_genders = next(image_generator)
        yield [batch_images, gender_values[:batch_size]], target_values[:batch_size]

print(train_df)

from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Input, GlobalMaxPooling2D, Dense, Flatten, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.applications.xception import preprocess_input
import pandas as pd
from sklearn.model_selection import train_test_split
# Define the gender input
gender_input = Input(shape=(1,), name='gender_input')  # Assuming you have a binary gender value (0 or 1)
gender_dense = Dense(10, activation='relu')(gender_input)

# Define the image input
image_input = Input(shape=(img_size, img_size, 3), name='image_input')
xception_model = tf.keras.applications.xception.Xception(input_shape=(img_size, img_size, 3),
                                           include_top=False,
                                           weights='imagenet')
xception_output = xception_model(image_input)
global_max_pooling = GlobalMaxPooling2D()(xception_output)
image_flatten = Flatten()(global_max_pooling)

# Concatenate the image and gender branches
merged = concatenate([image_flatten, gender_dense])

# Additional Dense layers for final prediction
merged_dense = Dense(10, activation='relu')(merged)
output = Dense(1, activation='linear')(merged_dense)

# Create the model
combined_model = Model(inputs=[image_input, gender_input], outputs=output)

# Compile the model
combined_model.compile(loss='mse', optimizer='adam', metrics=[mae_in_months])

# Display model summary
combined_model.summary()

# Commented out IPython magic to ensure Python compatibility.

# Load the TensorBoard notebook extension
# %load_ext tensorboard
logs_dir = '/content/drive/MyDrive/AI Project/Log3'
# %tensorboard --port 6007 --logdir "{logs_dir}"

# Commented out IPython magic to ensure Python compatibility.
#early stopping
early_stopping = EarlyStopping(monitor='val_loss',
                              min_delta=0,
                              patience= 5,
                              verbose=0, mode='auto')

#model checkpoint
mc = ModelCheckpoint('/content/drive/MyDrive/AI Project/Model/best_model3.h5', monitor='val_loss', mode='min', save_best_only=True)

#tensorboard callback
logdir = os.path.join(logs_dir,datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
tensorboard_callback =  TensorBoard(logdir, histogram_freq = 1)

#reduce lr on plateau
red_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)

callbacks = [tensorboard_callback,early_stopping,mc, red_lr_plat]
batch_size=32

#fit model
history = combined_model.fit(
    custom_train_generator,
    validation_data=custom_data_generator(val_image_generator, df_valid['gender'].values, val_image_generator.next()[1], batch_size),
    steps_per_epoch=10,
    validation_steps=1,
    epochs=50,
    callbacks=callbacks
)

history
# %tensorboard --logdir logs
plot_it(history)

# Train the model
history = combined_model.fit(
    x=[train_image_generator.next()[0], df_train['gender'].values],  # Provide both image and gender inputs
    y=train_image_generator.next()[1],  # Use the bone age Z-scores as target
    validation_data=(
        [val_image_generator.next()[0], df_valid['gender'].values],
        val_image_generator.next()[1]
    ),
    epochs=50,
    steps_per_epoch=len(train_image_generator),
    validation_steps=len(val_image_generator)
)

from tensorflow.keras.layers import GlobalMaxPooling2D, Dense, Flatten
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.applications import InceptionV3  # Import InceptionV3 directly
from tensorflow.keras import Sequential

# Define input shape and other relevant variables
img_size = 256  # Adjust this according to your needs

# Create the InceptionV3 base model
model_3 = tf.keras.applications.xception.Xception(input_shape=(img_size, img_size, 3),
                         include_top=False,
                         weights='imagenet')

model_3.trainable = True

# Build your custom model
model_4 = Sequential()
model_4.add(model_3)
model_4.add(GlobalMaxPooling2D())
model_4.add(Flatten())
model_4.add(Dense(10, activation='tanh'))
model_4.add(Dense(1, activation='linear'))

# Compile the model
model_4.compile(loss='mse', optimizer='adam', metrics=[mae_in_months])

# Display model summary
model_4.summary()